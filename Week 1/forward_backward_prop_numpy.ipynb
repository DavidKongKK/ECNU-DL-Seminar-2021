{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n请大家使用numpy库完成relu, derivation_relu, sigmoid三个函数的填空，以及forward、backward和train中部分功能的实现\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "请大家使用numpy库完成relu, derivation_relu, sigmoid三个函数的填空，以及forward、backward和train中部分功能的实现\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        z: (batch_size, hidden_size)\r\n",
    "    return:\r\n",
    "        a: (batch_size, hidden_size)激活值\r\n",
    "    \"\"\"\r\n",
    "    a=np.maximum(z,0)\r\n",
    "    \r\n",
    "    return a\r\n",
    "\r\n",
    "def derivation_relu(z):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        z: (batch_size, hidden_size)\r\n",
    "    return:\r\n",
    "        dz: (batch_size, hidden_size)导数值\r\n",
    "    \"\"\"\r\n",
    "    dz=z.copy()\r\n",
    "    dz[dz<=0]=0\r\n",
    "    dz[dz>0]=1\r\n",
    "    return dz\r\n",
    "\r\n",
    "def sigmoid(z):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        z: (batch_size, hidden_size)\r\n",
    "    return:\r\n",
    "        a: (batch_size, hidden_size)激活值\r\n",
    "    \"\"\"\r\n",
    "    a=1/(1+np.exp(-z))\r\n",
    "    return a\r\n",
    "\r\n",
    "def bi_cross_entropy(y, y_hat):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        y: (batch_size, ) 每个样本的真实label\r\n",
    "        y_hat: (batch_size, output_size)， 网络的输出预测得分，已经过sigmoid概率化。output_size即分类类别数\r\n",
    "    return:\r\n",
    "        loss: scalar\r\n",
    "    \"\"\"\r\n",
    "    n_batch = y_hat.shape[0]\r\n",
    "    loss = -np.sum(np.log(y_hat)) / n_batch\r\n",
    "    return loss\r\n",
    "def derivation_sigmoid_cross_entropy(y, y_hat):\r\n",
    "    \"\"\"\r\n",
    "    Args:\r\n",
    "        logits: (batch_size, output_size)， 网络的输出预测得分, 还没有进行 softmax概率化\r\n",
    "        y: (batch_size, ) 每个样本的真实label\r\n",
    "    \r\n",
    "    Return:\r\n",
    "        \\frac {\\partial C}{\\partial z^L}\r\n",
    "        (batch_size, output_size)\r\n",
    "    \"\"\"\r\n",
    "    y_hat -= 1\r\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\r\n",
    "    \"\"\"\r\n",
    "    fully-connected neural network\r\n",
    "    Attributions:\r\n",
    "        sizes: list, 输入层、隐藏层、输出层尺寸\r\n",
    "        num_layers: 神经网络的层数\r\n",
    "        weights: list, 每个元素是一层神经网络的权重\r\n",
    "        bias: list, 每个元素是一层神经网络的偏置\r\n",
    "        dws: list，存储权重梯度\r\n",
    "        dbs: list，存储偏置梯度\r\n",
    "        zs: list，存储前向传播临时变量\r\n",
    "        _as：list，存储前向传播临时变量\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, sizes):\r\n",
    "        self.sizes = sizes\r\n",
    "        self.num_layers = len(sizes)\r\n",
    "        self.weights = [np.random.randn(i, j) for i, j in zip(self.sizes[:-1], self.sizes[1:])]\r\n",
    "        self.bias = [np.random.randn(1, j) for j in self.sizes[1:]]\r\n",
    "        self.dws = None\r\n",
    "        self.dbs = None\r\n",
    "        self.zs = [] \r\n",
    "        self._as = []\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "        前向传播\r\n",
    "        x: (batch_size, input_size)\r\n",
    "        \"\"\"\r\n",
    "        a = x\r\n",
    "        self._as.append(a)\r\n",
    "        for weight, bias in zip(self.weights[:-1], self.bias[:-1]):\r\n",
    "            # 计算临时变量z和a并存入self.zs和self._as\r\n",
    "            z=np.dot(a,weight)+bias #TODO:用dot还是* ?\r\n",
    "            self.zs.append(z)\r\n",
    "            a=sigmoid(z)\r\n",
    "            self._as.append(a)\r\n",
    "            #########################################\r\n",
    "        logits = np.dot(a, self.weights[-1]) + self.bias[-1]\r\n",
    "        y_hat = sigmoid(logits)\r\n",
    "        self.zs.append(logits)\r\n",
    "        self._as.append(y_hat)\r\n",
    "        \r\n",
    "        return y_hat\r\n",
    "\r\n",
    "    def backward(self, x, y):\r\n",
    "        \"\"\"\r\n",
    "        反向传播\r\n",
    "        Args:\r\n",
    "            x: (batch_size, input_size)\r\n",
    "            y: (batch_size, )\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        y_hat = self.forward(x)\r\n",
    "        loss = bi_cross_entropy(y, y_hat)\r\n",
    "\r\n",
    "        ################# 反向传播梯度计算 ##############################\r\n",
    "        # 输出层误差\r\n",
    "        dl = derivation_sigmoid_cross_entropy(y, y_hat)\r\n",
    "        # batch的大小\r\n",
    "        n = len(x)\r\n",
    "        # 最后一层的梯度\r\n",
    "        # 每个样本得的梯度求和、求平均\r\n",
    "        self.dws[-1] = np.dot(self._as[-2].T, dl) / n\r\n",
    "        self.dbs[-1] = np.sum(dl, axis=0, keepdims=True) / n\r\n",
    "        # 计算梯度\r\n",
    "        for i in range(2, self.num_layers):\r\n",
    "            # 计算梯度并存入self.dws和self.dbs，注意矩阵乘法和逐元素乘法\r\n",
    "            self.dws[i]=np.dot(self._as[i-1].T,dl)/n\r\n",
    "            self.dbs[i]=np.sum(dl,axis=0,keepdims=True)/n\r\n",
    "            ############################################################\r\n",
    "            \r\n",
    "        self.zs = [] \r\n",
    "        self._as = []\r\n",
    "    \r\n",
    "    def zero_grad(self):\r\n",
    "        \"\"\"清空梯度\"\"\"\r\n",
    "        self.dws = [np.zeros((i, j)) for i, j in zip(self.sizes[:-1], self.sizes[1:])]\r\n",
    "        self.dbs = [np.zeros((1, j)) for j in self.sizes[1:]]\r\n",
    "        \r\n",
    "    def optimize(self, learning_rate):\r\n",
    "        \"\"\"更新梯度\"\"\"\r\n",
    "        self.weights = [weight - learning_rate * dw for weight, dw in zip(self.weights, self.dws)]\r\n",
    "        self.bias = [bias - learning_rate * db for bias, db in zip(self.bias, self.dbs)]\r\n",
    "\r\n",
    "        \r\n",
    "def train():\r\n",
    "    \r\n",
    "    n_batch = 5\r\n",
    "    n_input_layer = 2\r\n",
    "    n_hidden_layer = 3\r\n",
    "    n_output_layer = 1\r\n",
    "    n_class = 2\r\n",
    "    x = np.random.rand(n_batch, n_input_layer)\r\n",
    "    y = np.random.randint(0, n_class, size=n_batch)\r\n",
    "    net = Network((n_input_layer, n_hidden_layer, n_output_layer))\r\n",
    "    print('initial weights:', net.weights)\r\n",
    "    print('initial bias:', net.bias)\r\n",
    "    # 执行梯度计算\r\n",
    "    \r\n",
    "    net.zero_grad()\r\n",
    "    net.forward(x)\r\n",
    "    net.backward(x,y)\r\n",
    "    net.optimize(0.1)\r\n",
    "    \r\n",
    "    ##############\r\n",
    "    print('updated weights:', net.weights)\r\n",
    "    print('updated bias:', net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights: [array([[ 0.30852011,  1.09501105, -0.09601117],\n",
      "       [-0.23223195, -0.22939472, -0.98174297]]), array([[0.13609813],\n",
      "       [1.29496327],\n",
      "       [1.05000308]])]\n",
      "initial bias: [array([[ 0.55446742, -0.02541923, -0.37524321]]), array([[-0.84966087]])]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-483121c6450b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-483121c6450b>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# 计算梯度并存入self.dws和self.dbs，注意矩阵乘法和逐元素乘法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b609c7b4c75a045aebe1253cca1961aeae8713bd74d4c3e5d1343b5f2aa66b91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}